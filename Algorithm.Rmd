--- 
title: "Finding $k$ for $k$-means clustering"
author: "Arjun Poddar"
date: "January 01, 2016"
output: pdf_document
---

[k-means clustering](https://en.wikipedia.org/wiki/K-means_clustering) is a very popular method to find clusters in a set of data. 

In k-means clustering these $n$ data points or vectors are devided or grouped into $k$ groups or "clusters" based on how similar the data points are measured by the $d$-dimensions. Generally, this "similarity" between the points are calculated by using distance functions, for example the [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance).

Suppose we have a set of $n$ data points(observations) $(x_1, x_2, \dots, x_n)$, where each $x_i$ is a point in a $d$-dimensional space. This means that $x_i$ is the vector $(x_{i1}, x_{i2}, \dots, x_{id})$, for $i = 1, 2, \dots, n$. Let there be $k$ clusters, namely $S_1, S_2, \dots, S_k$. We define the following quantities:

\begin{eqnarray*}
&& \bar x_{.j} = \frac {1} {n} \sum_{i=1}^n x_{ij},\; \mbox{the mean of the $j^{th}$ component for the entire data}, j =1,2,\dots,d. \\
&& \mu_{lj} = \frac{1} {n_l} \sum_{x_i \in S_l} x_{ij},\; \mbox{the mean(center) of the $j^{th}$
component in the $l^{th}$ cluster}, j =1,2,\dots,d, \; l=1,2\dots,k.
\end{eqnarray*}

\begin{flalign*}
\mbox{The {\bf total sums of the sqaures} (TSS) of the data is given by, TSS}
= \sum_{i=1}^n \sum_{j=1}^d (x_{ij} - \bar x_{.j})^2
\end{flalign*}

Therefore, based on the centers of the $k$ clusters, TSS can be written as
\begin{flalign*}
\sum_{i=1}^n \sum_{j=1}^d (x_{ij} - \bar x_{.j})^2 =
\sum_{l=1}^k \sum_{x_i \in S_k} \sum_{j=1}^d (x_{ij} - \mu_{lj})^2 +
\sum_{l=1}^k \sum_{j=1}^d (\bar x_{.j} - \mu_{lj})^2 
\end{flalign*}

\begin{flalign*}
&\sum_{l=1}^k \sum_{x_i \in S_k} \sum_{j=1}^d (x_{ij} - \mu_{lj})^2
\mbox{is called the {\bf Within Cluster Sum of Squares} (WCSS) as it measures the sum} \\
&\mbox{of square of the distances of all the points from the respective centers of the clusters  they belong to}. \\ 
&\sum_{l=1}^k \sum_{j=1}^d (\bar x_{.j} - \mu_{lj})^2
\mbox{is called the {\bf Between Cluster Sum of Squares} (BCSS) as it measures the sum} \\
&\mbox{of square of the distances of all the centers of the clusters from the center of the entire data}.
\end{flalign*}
$$\mbox{Therefore, TSS = BCSS + WCSS } $$

\newpage

When the value of $k$ is known, here is, in a nutshell, how the k-means clustering algorithm works: 

&nbsp;

*  **Assignment**: Each of the $n$ points of data are assigned to any of the $k$ groups in such a way that the  WCSS is minimized. 

*  **Update**: The centers for all the clusters are updated using the points in allocated to them.




In [R](https://www.r-project.org/), the function [kmeans](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/kmeans.html) can be used to perform k-means clustering on a data set. The two primary arguements of this function are namely $x$ and centers, where $x$ is the numeric data in appropriate form (points or observations arranged in rows and the different dimensions of the observations in columns) and centers is k(the number of clusters) or a set of initial seed for the distinct $k$ centers.


## Finding the right k

If the number of clusters is known, $k$-means clustering is easy to do. But more often than not, $k$ is unknown because the distribution of the data may be unknown. If $k$ is too small with respect to the size of the data, clustering may not capture the true heterogeneity present in the data. On the other hand, if $k$ is too large a number, using the clustering information may not be useful at all. So, we have to find a balance.


If we change $k$, WCSS changes (for obvious reasons) but TSS does not (ofcourse). Let us verify this with a simple R code. We will use the numeric columns from the famous [Iris](https://en.wikipedia.org/wiki/Iris_flower_data_set) data set.

```{r}
# set the data
df <- iris[, 1:4]
#check the data
head(df)

# k-means clustering with k = 7
iris.cluster.7 <- kmeans(df, 7)
# k-means clustering with k = 11
iris.cluster.11 <- kmeans(df, 11)

#check the TSSs from the two clusterings 
cbind(iris.cluster.7$totss, iris.cluster.11$totss)
#check the WCSSs from the two clusterings 
cbind(iris.cluster.7$tot.withinss, iris.cluster.11$tot.withinss)
#check the BCSSs from the two clusterings 
cbind(iris.cluster.7$betweenss, iris.cluster.11$betweenss)
```
From the last three outputs, it is clear that though the TSS is constant regardless of what $k$ we choose, WCSS and BCSS are not. For each k-means clustering WCSS is calculated by calculating the sums of squares of distance of each point from the center of the cluster it belongs to. BCSS is obtained from subtracting WCSS from TSS.

&nbsp;

A smaller WCSS implies a better clustering of the data points. **So to choose a $k$ that gives us better result, we might look for a smaller WCSS or a bigger BCSS. 
As a criteria to choose the best $k$, we will run the k-means clustering for different values of $k$ and choose the one which has the highest BCCS** and we will plot the BCSS/TSS. We choose BCSS instead of WCSS because **the ratio BCSS/TSS is equivalent to [$R^2$, coeffieint of determination](https://en.wikipedia.org/wiki/Coefficient_of_determination)**. We call this ratio "variance explained" by the clustering.

&nbsp;

# Algorithm

  * For $k$ in the range (k.min, k.max):
    * Run $k$-means algorithm for $k$ clusters.
    * Calculate variance_explained[k] = BCSS/TSS.
  * Return $k$ for which variance_explained is highest.
  

# Implementation in R
So we write a function in R named **cluster.k** which takes three arguements - 

  1. data(name of the data frame)
  2. k.min (minimum value of $k$), default value = 2
  3. k.max(maximum value of $k$), default value = $ceiling(\sqrt{(n/2)})$


When we run this function on a dataset, a plot of percentage of variance explained and $k$ is produced along with a datafrane named variance.explained and a numeric value called $k$ for which the variance is maximized.

The R code for the function **cluster.k** can be found [here](https://github.com/ArjunPoddar/Finding_k_for_k-means_Clustering/blob/master/finding_k.R).

&nbsp

We run an example on the numeric columns of the Iris data set:
```{r echo=FALSE}
source('finding_k.R')
```


```{r}
cluster.k(iris[,1:4], 5, 25)
```
